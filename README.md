# HumanTalk LLM Benchmark

**Author:** ntcd_lol

**Version:** 1.0

**Testing Language:** Russian

## ğŸ¯ What is this benchmark about?

Most tests evaluate LLMs on logic, knowledge, or code generation. HumanTalk focuses on something different â€” the model's ability to conduct natural, empathetic, and supportive dialogue in a crisis situation.

Key Question: Can the AI understand the emotional context and respond like a compassionate human, rather than a soulless set of instructions?

## ğŸ“Š Test Results (September 2025)

![Results](results.png)
Visualization of the test results. Black zone â€” anti-result, white zone â€” adequate models.

## ğŸ† Key Findings

- DeepSeek-V3.1 (4/3) â€” Achieved a PERFECT result, exceeding expectations with its depth of empathy
- Qwen3-Max (3/3) â€” Perfect balance between logic and support
- Grok-4 (-1/3) â€” Worst result - Template filter without support

## ğŸ“‹ Methodology

- Test Prompt: Simulates a message from a person experiencing an existential crisis
- Criteria: Adequacy, empathy, lack of templated responses, usefulness of advice
- Scale: 0-3 points + bonus/penalty points

## ğŸ’¡ Main Takeaway

**Model power â‰  dialogue quality.** Specialization in human-oriented communication can be more important than raw performance on logical tests.

---

> "Numbers are great, but sometimes it's important to just hear a person out." â€” ntcd_lol
